Sure. Two items, which was, uh,
digits and  possibly  stuff on - on, uh, forced alignment, which  Jane  said that Liz and Andreas had in- information on, but they didn't, so.
I mean that it was basically - the only thing that was even slightly surprising was that the lapel did so well.
uh, I mean,  there  the point of interest to the group was  primarily  that, um,
the, uh - the system that we had that was based on H_T_ K,  that's used by, you know,   all  the participants in Aurora,
was so much  worse
than the - than the S_R_ I.  And the interesting thing is that even though,
yes, it's a digits task and that's a relatively small number of words and
there's a bunch of digits that you train on,
it's just  not  as  good
as having a - a l- very large amount of data and training up a - a - a nice good big
H_M_M. Um, also you had the adaptation in the S_R_I system, which we didn't have in this.
So there was a significant loss from not doing the adaptation.
Yeah, bu- although I'd be - I think it'd be interesting to just take this exact actual system so that these numbers were comparable and try it out on T_I-digits.
one - so there were a number of things we noted from this. One is, yeah, the S_R_I system is a lot better than the H_T_K -
Uh, but the other is that, um, the digits
recorded here in this room with these close mikes, i- uh, are actually a lot  harder  than the  studio-recording T_I-digits.
Well. But   remember,  we're using a  telephone  bandwidth front-end here, uh, on this, uh -
on this S_R_I system, so,
I suspect that to get sort of the last bit out of these higher-quality recordings you would have to in fact, uh,
use models that, uh, were trained on wider-band
data. And of course we can't do that or -
Right. Uh, but I'm not so much worried about the adaptation, actually, than - than the, um,
the, uh,
V_T_L estimation.
If you have only one utterance per speaker you might actually screw up on estimating the -
the warping, uh, factor. So, um -
uh, I_D or something that can stand in as a speaker I_D. So,
we might have to modify that script to recognize the, um, speakers,
um, in the - in the, uh, um,  T_I-digits  database.
I might have to do that anyway to - to do - because we may have to do an extract to get the  amount of data per speaker about right.
By the way, I think we can improve these numbers if we care to compr- improve them
by, um,  not starting with the Switchboard models but by taking the Switchboard models and doing supervised adaptation
on a small amount of digit data collected
in this setting.
you - you don't have any - That's where the most m- acoustic mismatch is between the currently used models and the - the r- the set up here. So.
But I think it's an important data point,
if you're - if - Yeah.
The other thing that - that, uh - of course,
what Barry was looking at was - was just that, the near versus far. And, yeah, the adaptation would get
little under a factor or two.
Yeah. I - I know what I was thinking was that maybe, uh,
we
could  actually t- t-  try  at least  looking  at, uh, some of the - the large vocabulary speech from a far microphone, at least from the  good  one.
Yeah, sure. Get all these insertions. But I'm saying if you do the same kind of limited thing
as people have done in Switchboard evaluations or as - a-
Could we do  exactly  the same thing that we're doing now, but do it with a far-field mike?
Cuz we extract the  times  from the near-field mike, but you use the  acoustics  from the far-field mike.
In the H_L_ T  paper we took  segments that are channel -  time-aligned,  which is now h- being changed in the  transcription  process, which is good,
and we took cases where the  transcribers  said there was only one person talking here, because no one else had time -
any  words in that segment and called that "non-overlap".
And that's what we were getting those numbers from. Right.
Tho- good - the good numbers. The bad numbers were from  the segments where there was overlap.
Well, we could  start  with the good ones. But anyway - so I think that we should try it once with
the same conditions that were used to create  those,  and in those same segments just use one of the P_Z_Ms.
You want to probably choose the P_Z_M channel that is closest to the speaker.
You know, it's - so i- but I would - I'd pick that one. It'll be less good for some people than for other, but I - I'd like to see it on the same - exact same data set that - that we did the other thing on. Right?
I sh- actually  should've  picked a different one, because  that  could  be why the P_D_A is worse.
Because it's further away from most of the people reading digits.
So - so, but where is this now? I mean, what's - where do we go from here? I mean,
Well, I think what we wanna do is we want to - eh, and we've talked about this in other  contexts - we want to
have the ability to feed it different features.
And then, um,
from the point of view of the front-end research, it would be s- uh, substituting for H_T_K.
And then, um,
uh, also Dave is - is thinking about
using the data in different ways, uh, to
um,
uh,
explicitly work on reverberation starting with some techniques that some other people have  found somewhat useful, and - Yeah.
So - so the key  thing that's missing here is basically the
ability to feed,
you know, other features  i- into the recognizer and also then to train the system.
It's - uh, I mean, the - the front-end is f- i- tha- that's in the S_R_I recognizer is very nice in that it
does a lot of things on the fly but it unfortunately  is not  designed and, um -  like the, uh, ICSI system is, where you can feed it
from a pipeline of - of the command. So,
the - what that means probably for the foreseeable future is that
you have to, uh, dump out, um -
you know, if you want to use some new features, you have to dump them into
individual files
We do - we tend to do that anyway.
Yeah, the - the - the cumbersome thing is - is, um - is that you actually have to dump out little - little files. So for each segment that you want to recognize  you have to  dump out  a separate file.
I can give a - I was just telling this to  Jane  and - and -
W- we -
we were able to get some
definite improvement on the forced alignments by
looking  at them first and then realizing the kinds of errors  that were occurring and
Actually it was  better  with - slightly better or about th- it was the  same  with tighter pruning.
So for free recognition, this - the lower pruning value is  better.  You -
Um, but it turned out for - for - to get accurate  alignments  it was really important to open up the pruning
significantly.
Um,  so that was one big factor that helped improve things and then the other thing was that,
you know, as Liz said the - we f- enforce
the fact that,
uh, the foreground speech has to be continuous. It cannot be -
Yeah. I mean, yeah, it isn't  always  true, and I think what we really want is some  clever  way to do this, where,
um, you know, from the  data  or from maybe some hand-corrected alignments from transcribers that
things like
words that  do  occur just by themselves  a- alone, like backchannels or something that we  did  allow to have background speech around it -
those  would be able to do that, but the rest would be constrained.
So, I think we have a version that's
pretty good for the native speakers. I don't know yet about the non-native speakers.
Um, so, and then there's a background speech model.
u-
We probably want to adapt
at least the foreground speaker. But, I guess Andreas tried adapting both the foreground and a background generic speaker, and that's actually a little bit
of a f- funky model. Like, it gives you some weird alignments, just because often the  background  speakers match better to the foreground than the foreground speaker. So there's some things  there,  especially when you get lots of the same words,
And you - and what we wanted to try with - you know, once we have this paper written and have a little more time,
uh, t- cloning that reject model and then one copy of it would be adapted to the foreground speaker
to capture the rejects in the  foreground,  like fragments and stuff, and the other copy would be adapted to the background speaker.
and so this constraint of not allowing rejects in between - you know, it needs to differentiate between the two. So just sort of working through a bunch of
debugging kinds of issues.
And really the reason we can't do it is just that we don't have a - we don't have ground truth for these. So,
we would need a hand-marked, um,  word-level alignments or at least sort of the boundaries of the speech betw- you know, between the speakers.
and then use that as a reference and tune
the parameters of the - of the model, uh, to op- to get the best  performance.
you know, have  time  to l-  to look at all of them and it would be  really  useful to have, like, a - a transcriber who could use Waves,
just in terms of the  tool,  talking about this. I guess  Sue  had had some  reactions. You know, interface-wise if you're looking at speech, you wanna be able to know really where the words are. And so,
we can give you some examples of sort of what this output looks like, um, and see if you can in-
maybe incorporate it into the Transcriber tool some way,  or -
It  seems  like she - if she's g- if she's moving  time  marks around, since our representation in Transcriber uses  time  marks, it seems like there should be  some  way of - of  using  that -  benefitting  from that.
Yeah, it wou- the advantage would just be that when you brought up a bin you  would  be able - if you were zoomed in  enough
in Transcriber to see all the words, you would be able to, like, have the words sort of located in time,
So we - we only r- hav- I only  looked  at actually alignments from  one  meeting that we chose, I think M_R four, just randomly, um -
Well, I know there were some speaker labeling problems, um, after interruptions. Is that what you're referring to?
Fixed that.
O_K. But you're actually saying that certain, uh, speakers were mis- mis-identified.
and embedde- embedded in overlaps. The  other  thing that was w- interesting to me was
that I picked up a lot of, um,  backchannels  which were hidden in the mixed signal, which, you know, I mean, you c- not - not too surprising.
When I was looking at these backchannels,
they were turning up usually -
very  often in - w- well, I won't say "usually" - but anyway, very often, I picked them up in a channel
w- which was the person who had asked a question.  S- so, like, someone says "an- and have you done the so-and-so?"
But there are  fewer  - I think there are fewer "uh-huhs". I mean, just from - We were looking at word frequency lists to try to find the cases that we would  allow  to have the  reject  words in  between  in doing the  alignment.
as it sort of would be in Switchboard, if you looked at just a word frequency list of one-word
short utterances. And " yeah " is way up there, but not "uh-huh". And so I was thinking
in the meeting, but we'll find out anyway. We were - I guess the other thing we're - we're - I should say is that we're gonna, um
try - compare this type of  overlap  analysis to  Switchboard,
where - and  CallHome,  where we  have  both sides, so that we can try to answer this question of, you know,
is there really  more   overlap  in  meetings  or is it just because we don't have the other channel in  Switchboard  and we don't know what people are doing.
Yeah. I mean, y- y- you folks have probably  already told me, but were - were you intending to do a Eurospeech submission, or - ?
Well, I'm no- We may be in the same position, and I figured
we'll  try,  because that'll at least get us to the point where we have - We have this really nice database format that Andreas and I were working out that -
It - it's not very  fancy.  It's just a
ASCII line by line format, but it  does  give you information -
It's the - it's the spurt format.
It - Yeah, we're calling these "spurts" after Chafe. I was trying to find what's a word for  a continuous region with  pauses around it?
from each alignment we're producing, uh, one of these C_T_M files, which essentially has - it's just a linear sequence of words with the begin times for every word and the duration.
And the second column is the channel.
Third column is the, um,  start  times of the words and the fourth column is the  duration  of the words.
O_K. Then we have a messy alignment process where we actually insert into the sequence of words
the, uh,  tags  for, like, where - where sentence - ends of sentence, question marks, um,
And then we merge all the alignments from the various channels and we sort them by time.
And so you extract the individual  channels,  uh, one sp- spurt by spurt as it were.
Um, and inside the words or between the words you now have begin and end  tags for overlaps. So,
you - you basically have everything sort of lined up and in a form where you can look at the individual speakers and how their speech relates to the other speakers' speech.
Uh, I mean, I think that's actually really u- useful  also  because
even if you weren't studying overlaps, if you wanna get a transcription for the far-field mikes, how are you gonna  know
which words from which speakers occurred at which times  relative  to each other? You  have  to be able to  get a transcript like - like this anyway, just for doing far-field recognition. So,
at the very last stage we throw away the actual  time  information. All we care about is whether -
that there's a certain word was overlapped by someone  else's  word.
So you sort of - at that point, you discretize things into just having overlap or no overlap.
Because we figure that's about the level of analysis that we want to do for this paper.
Yeah. So tha- so we'll - you know, maybe you guys'll have - have one. Uh, you - you and, uh - and Dan have - have a paper that -
that's going in. You know, that's - that's pretty solid, on the segmentation  stuff.
And the Aurora folks here will - will definitely get something in on Aurora, so.
th- the  other  good thing about the alignments is that, um, it's not always the  machine's  fault if it doesn't work.  So, you can actually find, um,
You can find, uh, problems with - with the transcripts, um, you know,
Tha- There are some cases like where the - the wrong speaker - uh, these ca-
Not  a  lot,  but where the - the wrong person - the - the speech is addre- attached to the wrong speaker and you can tell that when you run it.
