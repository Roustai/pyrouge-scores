So. On Friday we had our wizard test data test
and um
these are some of the results.
serve as the first subject. So, this is what she  saw  as part of - as uh for instr- introduction, this is what she had to  read
aloud.
this was the uh first three tasks she had to - to master after she  called  the system,
uh I should say the system was  supposed  to break down and then um these were the remaining three tasks that she was going to  solve,  with a  human  -
And um.
The  reading  was five minutes, exactly. And now comes the - This is the phone-in phase of -
So there's  no  system, right? Like, there was a wizard for both uh - both parts, is this right?
Yeah. It was bo- it both times the same person.
One time,  pretending  to be a system, one time, to -  pretending  to be a  human,  which is actually not pretending. I should -
lessons learned.  The reading needs to be shorter. Five minutes is just too long.
Um,
that was already anticipated by  some  people suggested that if we just have bullets here, they're gonna not - they're - subjects are probably not gonna - going to follow the order. And uh  she  did not. She - No. She - she jumped around quite a bit.
S- so if you just number them "one", "two", "three" it's
Um. We need to - So that's one thing. And we need a better introduction for the wizard. That is something that Fey actually thought of a - in the last second that sh- the system should introduce  itself,  when it's called.
And um, um, another suggestion, by  Liz,  was that we uh, through subjects, switch the tasks. So when - when they have task-one with the computer, the next person should have task-one with a human, and so forth. So we get nice um data for that.
Um, we have to refine the tasks more and more, which of course we haven't done at all, so far,
in order to avoid this  rephrasing,  so where, even though w- we don't tell the person "ask  blah-blah-blah-blah-blah"
And uh my suggestion is of course we - we keep the wizard, because I think she did a wonderful job, in the sense that she responded quite nicely to things that were not
Oh, yeah, and then of course as she does it she'll - she'll learn  @@ . So that's great. Um
And also if she's willing to take on the job of organizing all those subjects and stuff that would be wonderful.
And um I told her that we gonna um figure out a meeting time in the near future to refine the tasks and s- look for the potential sources to find people.
She also agrees that you know if it's all just gonna be students the data is gonna be less valuable because of that so.
Yeah. We could talk to the people who run it and um see if they have a way that they could easily uh tell people that there's a task, pays ten bucks or something, but
um
you have to be comfortable reading relatively complicated stuff. And - and there'll probably be self-selection to some extent.
so  that's  good. Um.
Now,  I signed us up for the Wednesday slot,
and part of what we should do is this. So,
my idea on that was
uh, partly we'll talk about system stuff for the  computer  scientists, but partly I did want it to get the  linguists  involved
in some of this issue about what the  task  is and
all - um you know, what the  dialogue  is, and what's going on  linguistically,
because to the extent that we can get  them   contributing,
that will be good.
to uh - you - I mean, what you did today would - i- is just  fine.  You just uh do "this is what we did, and here's the  thing, and here's s- some of the dialogue and - and
so forth."
So, what I did for this - this is - uh, a pedagogical belief-net because
So basically all I did was I took the last  belief-net
and I grouped things according to what - how I thought they would fit in to
uh  image  schemas that would be related. And the two that I came up with
were Trajector-landmark and then Source-path-goal as initial ones.
And then I said well, uh the trajector would be the person in this case probably.
Um,
you know, we have - we have the concept of what their intention was, whether they were trying to tour or do business or whatever,
or they were hurried.  That's  kind of related to  that.  And then um
Yeah, so -
um,
in terms of Context, what we had currently said was
whether they were a businessman or a tourist of some other
person.
Um, Discourse was related to whether they had asked about open hours or whether they asked about where the entrance was or the admission fee, or something along those lines.
Uh, Prosody I don't really - I'm not really sure what prosody means, in this context,
Um, the Parse would be what verb they chose, and then maybe how they modified it, in the sense of
whether they said "I need to get there quickly" or whatever.
And um, in terms of World Knowledge, this would just basically be like opening and closing times of things, the time of day it is, and whatnot.
Well, this is not a working Bayes-net.
And - and uh - Bhaskara and I was talking about this a little earlier today - is, if we just  do  this, we could wind up with a  huge
uh,
combinatoric input to the  Mode  thing.
Well I - oh yeah, I unders- I understand  that,  I just - uh it's hard for me to imagine how he could get around that.
Uh. Let me just mention something that I  don't  want to pursue today which is
there are  technical  ways of doing it, uh I- I slipped a paper to Bhaskara
and - about Noisy-OR's and Noisy-MAXes and
try to  informally  - I mean, not necessarily in th- in this  meeting,
but to try to  informally  think about what the decision variables  are.
uh decision about which  mode,
you know,  what  are the most relevant  things.
And the  other  trick, which is not a  technical  trick, it's kind of a  knowledge  engineering trick,
is to make the n-  each node sufficiently narrow
that you don't get this combinatorics. So
decision node just  above  the final one.
And  then  the question would be if - if  those  are the things that you  care  about,
uh can you make a  relatively  compact way of getting
from the various  inputs  to the things you  care  about.
But the uh -
the - the nice thing is that you know, it just
is a - is a visual aid for thinking about these things which has comple-
clearly have to be specified m- more carefully and uh
Alright, well, le- let me think about this some more,
and uh see if we can
find a way to
present this
to this  linguists  group that - that is  helpful  to  them.
uh throw the die for you,  because um
I integrated this into the  existing  SmartKom system in - in the same way as
much the same way we can um
sort of have this uh -
this  thing.
So, I - I  do  understand that
uh you can take the M_-three-L_ and
add not - and it w-
and you  need  to do this, for sure, we have to add, you know, not too much about
um
object types and stuff,
and what I  think  you did
is add some rules  of the style that are already  there  that say
"If it's of type "Landmark", then you
take - you're gonna take a picture of it."
Yeah.
And it - it would do us no good. That - Ultimately.
that's a - that's another kind of baseline case, that's another sort of thing "O_K,
here's a - another kind of minimal
uh way of tackling this".
Add extra properties, a deterministic rule for every property you have an action, "pppt!"
And the  rules  we want to throw away  completely.
And um - and here is exactly where what's gonna be replaced with our Bayes-net, which is exactly getting the input feeding into here.  This  decides
So that - that uh i- if you had the
generalized "Go" X_schema and you wanted to specialize it to these three ones, then you would have to supply the parameters.
So the immediate problem is just deciding w-
which -
Aspects of the X_schema to add.
Yeah, so the pro- The  immediate  problem is - is back t- t- to what you were - what you are doing with the belief-net. You know,
uh what are we going to use to make this decision -
Right. Well,  that  -  that  actually is relatively  easy  in this  case.
The  harder  problem is
we decide what we want to  use,  how are we gonna  get  it?
Um, and that's - so, getting back to here, uh, we have a d- a technical problem with the belief-nets that we - we
don't want all the com-
too many factors if we - if we allow them to just go combinatorially.
So the belief-net takes as input, a vector, right?
and then we want to look up some more stuff in the ontology and we want to look up some more stuff
in the - maybe we want to ask the real world, maybe you want to look something up in the G_R_S, but also we definitely want to
look up in the dialogue history um some s- some stuff. Based on we - we have uh - I was just made some examples from the ontology
and so forth, and maybe draw some inferences on that. So this may be a - a sort of a process of two to three steps before we get our vector, that we feed into the belief-net, and then -
Yeah. I think that's - I think that's exactly right.
There  will  be  rules,  but they aren't rules that come to final  decisions,  they're rules that gather information for a decision process.
My guess is it'll be the same basic agent that um
can go off and get information,
run it through a - a c- this belief-net that -
O_K,
which can then be
uh applied
at what we would call the simulation or action end. So you now know what you're gonna do and
that may actually involve getting  more  information. So on- once you pull that out, it could be that that says "Ah! Now that we know that we gonna go ask the ontology something else."
I think we - I - I can come up with a - a code for a module that we call the "cognitive dispatcher",
which does nothing, but it looks  of complect  object trees and decides how - are there parts missing that need to be filled out, there's - this is maybe something that this module can do, something that this module can do and then collect
uh sub-objects and then
recombine them and put them together.
So. What you're trying to get out of this deep co- cognitive linguistics
is the fact that w- if you know about source - source,  paths   and goals, and nnn  all this sort of stuff,
that a  lot  of this is the  same,
for different tasks. And that -
uh there's - there's some - some important generalities that you're  getting,  so that you  don't  take each and every one of these tasks and hafta re- do  it.
And I don't  yet  see how that  goes.
The Bayes-nets will be dec- specific for each decision. But what I'd like to be able to do
is to have the way that you extract properties,
that will go into different Bayes-nets, be
the - uh  general.
So that if you have sources, you have trajectors and stuff like that, and there's a language for  talking  about trajectors,
you  shouldn't  have to do that  differently
for uh uh going to something, than for circling it, for uh telling someone else how to go there,  whatever  it is.
What you'd  really  like of course is the same thing you'd  always  like which is that you have
um a kind of intermediate representation which looks the same o- over a bunch of inputs and a bunch of outputs.
Oh yeah, b- But an architecture like this would also enable us maybe to - to throw this away and - and replace it with something else, or whatever, so that we have - so that this is sort of the representational formats we're - we're - we're talking about that are independent of the problem,
that generalize over those problems, and are
oh, t- of a higher quality than an- any actual whatever um
belief-net, or
"X_" that we may use for the decision making,
ultimately.
